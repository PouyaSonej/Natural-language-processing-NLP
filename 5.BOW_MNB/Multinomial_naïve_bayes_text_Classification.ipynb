{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73a93d9-9cb0-46bf-b4c0-31248d009c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f8e92-36a6-4703-9ca2-5a394f15fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_p(n):\n",
    "    # Get the probability's n natural algorithm\n",
    "    return abs(math.log(abs(n))) \\\n",
    "        if n != 0 and n != 1 else 1\n",
    "\n",
    "def get_fea_class(D,k):\n",
    "    # Get all documents in D belonging to the k-th class in C\n",
    "    return np.array([ d for d in D if k == int(d[0]) ])\n",
    "\n",
    "def get_count_class(D,k):\n",
    "    # Get the count p(Ck) of the class Ck in documents D\n",
    "    return len(get_fea_class(D, k))\n",
    "\n",
    "def get_counts_term(D,w):\n",
    "    # Get the count of the term w occurrences in each document from D\n",
    "    count_wt = np.array([ len([ term \\\n",
    "        for term in d[1] if w == term ]) for d in D ])\n",
    "    # Get the total count of documents from D, containing the term w\n",
    "    return len(np.array([ f_wt \\\n",
    "        for f_wt in count_wt if f_wt > 0 ]))\n",
    "\n",
    "def get_prob_class(D,k):\n",
    "    # Get the probability p(Ck) of the k-th class Ck\n",
    "    return get_count_class(D,k) / len(D)\n",
    "\n",
    "def get_probs_term(D,w):\n",
    "    # Get the probability of the term w occurrence \n",
    "    # in each document from the class Ck\n",
    "    return get_counts_term(D,w) / len(D)\n",
    "\n",
    "def parse(S):\n",
    "    W = S.lower().split()\n",
    "\n",
    "    # Parse the string S, performing \n",
    "    # the normalization and word-stamming using NLTK library\n",
    "    W = np.array([ re.sub(r\"\"\"[,.;@#?!&$\\']+\\ *\"\"\", '', w) for w in W])\n",
    "    W = np.array([ tag[0] for tag in nltk.pos_tag(W) \\\n",
    "        if re.match('NN', tag[1]) != None or re.match('JJ', tag[1]) != None ])\n",
    "\n",
    "    return np.array([ w for w in W if len(w) > 2 ])\n",
    "    \n",
    "def build_model(D):\n",
    "    # Build the class prediction model, \n",
    "    # based on the corpus of documents in D\n",
    "    D = np.array([ np.array([ d[0], parse(d[1]) ], \\\n",
    "        dtype=object) for d in D ], dtype=object)\n",
    "    return np.array([ d for d in D if len(d[1]) > 0 ])\n",
    "\n",
    "def compute(D,C,S):\n",
    "    W = parse(S);                 # A set of terms W in the sample S\n",
    "    Pr = np.empty(0);             # A set of posteriors Pr(Ck | W)\n",
    "\n",
    "    n = len(W); m = len(C)        # n - # of terms W in S\n",
    "                                  # m - # of classes in C\n",
    "\n",
    "    # For each k-th class Ck, compute the posterior Pr(Ck | W)\n",
    "    for k in range(m):\n",
    "        pr_ck_w = 0                  # pr_ck_w - the likelihood P(Ck | wi) \n",
    "                                     # of Ck is the class of the term wi\n",
    "\n",
    "        d_ck = get_fea_class(D,k)    # d_ck - A set of documents from the class Ck\n",
    "        p_ck = get_prob_class(D,k)   # p_ck - Probability of the k-th class Ck in documents D\n",
    "\n",
    "        # For each term W[i], compute the likelihood P(Ck | wi)\n",
    "        for i in range(n):\n",
    "            # Obtain the count and probability of the \n",
    "            # term W[i] in the documents from class Ck\n",
    "            prob_wd_n = get_probs_term(d_ck, W[i])\n",
    "            count_wt_n = get_counts_term(d_ck, W[i])\n",
    "            \n",
    "            pr_ck_w += count_wt_n * \\\n",
    "                log_p(prob_wd_n) if count_wt_n > 0 else 0\n",
    "\n",
    "        pr_ck_w += p_ck\n",
    "\n",
    "        # Append the posterior Pr(Ck | W) of the class Ck to the array Pr\n",
    "        Pr = np.append(Pr, pr_ck_w)\n",
    "\n",
    "    # Obtain an index of the class Cs as the class in C, \n",
    "    # having the maximum posterior Pr(Ck | W)\n",
    "    Cs = np.where(Pr == np.max(Pr))[0][0]\n",
    "   \n",
    "    return Pr,Cs   # Return the array of posteriors Pr\n",
    "                   # and the index of sample S class Cs\n",
    "\n",
    "def evaluate(T,D,C):\n",
    "    print('Classification:')\n",
    "    print('===============\\n')\n",
    "\n",
    "    # For each sample S in the set T, compute the class of S\n",
    "    # Estimate the real classification's multinomial entropy and its expectation\n",
    "    for s in T[:,1]:\n",
    "        pr_s = '\\0'; \\\n",
    "            Pr,Cs = compute(D,C,s)\n",
    "        for ci,p in zip(range(len(C)),Pr):\n",
    "            pr_s += prob_stats % (C[ci][1],p)\n",
    "\n",
    "        print(sampl_stats % (s, C[Cs][1] \\\n",
    "            if np.sum(Pr) > 0 else 'None', pr_s))\n",
    "\n",
    "def load_data(filename):\n",
    "    cols_max = 2; \\\n",
    "       data = np.empty((0, cols_max))\n",
    "    filename = '..\\\\dataset\\\\' + filename\n",
    "    filename = os.path.dirname( \\\n",
    "        os.path.realpath(__file__)) + '\\\\' + filename\n",
    "    with open(filename, newline='\\n') as csvfile:\n",
    "        for line in csv.reader(csvfile, delimiter='_', \\\n",
    "            quotechar='', quoting=csv.QUOTE_NONE):\n",
    "                data = np.append(data, [line], axis=0)\n",
    "\n",
    "    return np.array(data)\n",
    "\n",
    "def output_data(T,D,C,fmt):\n",
    "\n",
    "    print(model_stats % \\\n",
    "        (len(C),len(D),len(T)))\n",
    "\n",
    "    print('Classes:')\n",
    "    print('========\\n')\n",
    "\n",
    "    for c in C:\n",
    "        k = int(c[0])\n",
    "        dc = get_fea_class(D,k); \\\n",
    "            p_ck = get_count_class(D,k) / len(D)\n",
    "        pd_stats = fmt % (len(dc), k + 1, p_ck)\n",
    "        print('C%d: %s %s' % (k + 1, \\\n",
    "            '{0: <12}'.format(c[1]), pd_stats))\n",
    "\n",
    "    print('\\n')\n",
    "\n",
    "    print('Documents:')\n",
    "    print('==========\\n')\n",
    "\n",
    "    for d in D:\n",
    "        print('C%d: \\\"%s...\\\"' % \\\n",
    "            (int(d[0]) + 1, d[1][:80]))\n",
    "\n",
    "    print(\"\\n\")\n",
    "    \n",
    "prob_stats  = 'Pr(%s) = %f '\n",
    "class_stats = '[ Documents: %3d, P(C%d) = %f ]'\n",
    "sampl_stats = 'Text: [ \\\"%s\\\" ]\\nClass: \\\"%s\\\" [%s]\\n'\n",
    "model_stats = '[ Classes: %d Documents: %d Samples: %d ]\\n'\n",
    "multi_stats = 'Multinomial Entropy: [ max: %f, real: %f ] classes/term\\n'\n",
    "\n",
    "app_banner  = '\\nMultinomial NaÑ—ve Bayes\\' Classifier | GNU License (C) 2021 | Arthur V. Ratz'\n",
    "\n",
    "def main():\n",
    "    print(app_banner)\n",
    "    print('===========================================================================\\n')\n",
    "\n",
    "    T = load_data('eval.txt'); \\\n",
    "        D = load_data('trainset.txt'); \\\n",
    "            C = load_data('classes.txt')\n",
    "\n",
    "    M = build_model(D)\n",
    "\n",
    "    output_data(T,D,C,class_stats); \\\n",
    "        entropy = evaluate(T,M,C);\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
