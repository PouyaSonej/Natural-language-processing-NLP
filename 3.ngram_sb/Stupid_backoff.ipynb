{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194e1fb-a134-4796-9a0c-ec0bf6f07abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk,io,sys,re\n",
    "from collections import Counter,defaultdict\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.util import ngrams\n",
    "import math, collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2cac9ad-aa68-48c2-85de-33c37cdb44f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,vocab = load_data(\"/home/pouya/RESOUCES#/NLP/T3/train1.txt\")\n",
    "valid_data,_ = load_data(\"/home/pouya/RESOUCES#/NLP/T3/valid1.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a80ed4-783e-45d7-981f-4ec76c366472",
   "metadata": {},
   "source": [
    "### 1 remove_rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9bd08919-3bb9-400e-9fd2-e2dfd3b3c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_words(data, vocab, mincount = 1):\n",
    "    '''\n",
    "    Parameters:\n",
    "    data (list of lists): each list is a sentence of the text \n",
    "    vocab (dictionary): {word: no of times it appears in the text}\n",
    "    mincount(int): the minimum count \n",
    "    \n",
    "    Returns: \n",
    "    data_with_unk(list of lists): data after replacing rare words with <unk> token\n",
    "    '''\n",
    "    # replace words in data that are not in the vocab \n",
    "    # or have a count that is below mincount\n",
    "    data_with_unk = []\n",
    "    ## FILL CODE\n",
    "    for sentence in data:\n",
    "        sentence_with_unk = []\n",
    "        for word in sentence:\n",
    "            if word in vocab and vocab[word]>mincount:\n",
    "                sentence_with_unk.append(word)\n",
    "            else:\n",
    "                sentence_with_unk.append('<unk>')\n",
    "        data_with_unk.append(sentence_with_unk)\n",
    "    \n",
    "    return data_with_unk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8fc43-c0d3-4a11-8245-a0479f32c93e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2 build_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e58b5e0-c47a-4af5-8c50-7d7b8e4ba0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ngram(data, n):\n",
    "    '''\n",
    "    Parameters:\n",
    "    data (list of lists): each list is a sentence of the text \n",
    "    n (int): size of the n-gram\n",
    "    \n",
    "    Returns:\n",
    "    proba (dictionary of dictionary)\n",
    "    {\n",
    "        context: {word:probability of this word given context}\n",
    "    }\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    total_number_words = 0\n",
    "    counts = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "\n",
    "    for sentence in data:\n",
    "        sentence = tuple(sentence)\n",
    "        ## FILL CODE\n",
    "        # dict can be indexed by tuples\n",
    "        # store in the same dict all the ngrams\n",
    "        # by using the context as a key and the word as a value\n",
    "        for i in range(len(sentence)):\n",
    "            total_number_words +=1\n",
    "            for k in range(n):\n",
    "                if i-k < 0:\n",
    "                    break\n",
    "                counts[sentence[i-k:i]][sentence[i]] +=1 \n",
    "                           \n",
    "\n",
    "    proba  = defaultdict(lambda: defaultdict(lambda: 0.0))\n",
    "    # Build the probabilities from the counts\n",
    "    # Be careful with how you normalize!\n",
    "\n",
    "    for context in counts.keys():\n",
    "    ## FILL CODE\n",
    "        denom =0\n",
    "        for w in counts[context].keys():\n",
    "            denom += counts[context][w]\n",
    "        for w in counts[context].keys():\n",
    "            proba[context][w] = counts[context][w]/denom \n",
    "    \n",
    "    return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede44ff8-4533-443a-ada9-b0fe29478046",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3 get_prob (our backoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1a80893-062b-4c6b-b821-4486f3aedaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob(model, context, w):\n",
    "    '''\n",
    "    Parameters: \n",
    "    model (dictionary of dictionary)\n",
    "    {\n",
    "        context: {word:probability of this word given context}\n",
    "    } \n",
    "    context (list of strings): a sentence\n",
    "    w(string): the word we need to find it's probability given the context\n",
    "    \n",
    "    Retunrs:\n",
    "    prob(float): probability of this word given the context \n",
    "    '''\n",
    "\n",
    "    # code a recursive function over \n",
    "    # smaller and smaller context\n",
    "    # to compute the backoff model\n",
    "    \n",
    "    ## FILL CODE\n",
    "\n",
    "    if context in model and w in model[context]:\n",
    "        return model[context][w]\n",
    "    else:\n",
    "        return 0.4*get_prob(model, context[1:], w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03cc2ac-135f-4d5d-9862-d8b121b78971",
   "metadata": {
    "tags": []
   },
   "source": [
    "## recalling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cf42e0ae-a23d-467f-ba47-9b9672b43955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove rare words\n"
     ]
    }
   ],
   "source": [
    "print(\"remove rare words\")\n",
    "train_data = remove_rare_words(train_data, vocab, mincount = 1)\n",
    "valid_data = remove_rare_words(valid_data, vocab, mincount = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5adadd4e-aa91-4313-882d-599b87530046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build ngram model with n =  4\n"
     ]
    }
   ],
   "source": [
    "# RUN TO BUILD NGRAM MODEL\n",
    "\n",
    "n = 4\n",
    "print(\"build ngram model with n = \", n)\n",
    "model = build_ngram(train_data, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1419910-ec6d-4a48-a11d-feafb51c51ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w is the desired string'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stupid backoff\n",
    "\n",
    "# get_prob(model, context, w)\n",
    "\n",
    "\"\"\"w is the desired string\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
